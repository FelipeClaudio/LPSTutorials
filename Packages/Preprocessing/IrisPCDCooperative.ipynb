{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tutorial de Extração de Componentes Principais de Discriminação no banco de dados Iris\n",
    "\n",
    "# Autor: Natanael Junior (natmourajr@gmail.com)\n",
    "# Laboratorio de Processamento de Sinais - UFRJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import libraries: 4.346 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import Libs\n",
    "import time\n",
    "init_time = time.time()\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['ytick.labelsize'] = 20\n",
    "\n",
    "current_time = time.time()\n",
    "print 'Time to import libraries: %1.3f seconds'%(current_time-init_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import data: 0.910 seconds\n"
     ]
    }
   ],
   "source": [
    "# Read Data\n",
    "from keras.utils import np_utils\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data # iris data\n",
    "trgt = iris.target # iris data\n",
    "iris_colors = ['b','r','g']\n",
    "iris_labels = ['Setosa','Versicolor','Virginica']\n",
    "\n",
    "# for classification -> target max sparse\n",
    "trgt_sparse = np_utils.to_categorical(trgt)\n",
    "\n",
    "current_time = time.time()\n",
    "print 'Time to import data: %1.3f seconds'%(current_time-init_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.callbacks as callbacks\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Merge\n",
    "\n",
    "\n",
    "def pcdc_extractor(inputdata, targetdata, trn_params=None):\n",
    "    ''' \n",
    "        This function extracts the Cooperative Principal Components of Discrimination of a Dataset\n",
    "        \n",
    "        Parameters:\n",
    "            inputdata: dataset with inputs\n",
    "            \n",
    "            targetdata: each class -> an integer\n",
    "            \n",
    "            trn_params: train parameters\n",
    "            \n",
    "            trn_params['n_folds'] = number of cross validation folds\n",
    "            trn_params['n_inits'] = number of initializations\n",
    "            trn_params['n_pcds'] = number of PCDs to be extracted\n",
    "            trn_params['norm'] = normalization\n",
    "            trn_params['learning_rate'] = learning rate\n",
    "            trn_params['learning_decay'] = learning rate decay\n",
    "            trn_params['momentum'] = momentum\n",
    "            trn_params['nesterov'] = nesterov momentum\n",
    "            trn_params['train_verbose'] = train verbose\n",
    "            trn_params['n_epochs'] = number of epochs\n",
    "            trn_params['batch_size'] = batch size\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    if trn_params == None:\n",
    "        trn_params = {}\n",
    "        trn_params['n_folds'] = 2\n",
    "        trn_params['n_inits'] = 2\n",
    "        trn_params['n_pcds'] = 2\n",
    "        trn_params['norm'] = 'none'\n",
    "        trn_params['learning_rate'] = 0.01\n",
    "        trn_params['learning_decay'] = 1e-6\n",
    "        trn_params['momentum'] = 0.3\n",
    "        trn_params['nesterov'] = True\n",
    "        trn_params['train_verbose'] = False\n",
    "        trn_params['n_epochs'] = 300\n",
    "        trn_params['batch_size'] = 8\n",
    "\n",
    "    print 'PCD Cooperative Extractor'\n",
    "    print 'trn_params: ',trn_params\n",
    "    \n",
    "    # trained classifiers\n",
    "    classifiers = {}\n",
    "    trn_desc = {}\n",
    "    pcds = {}\n",
    "    \n",
    "    CVO = cross_validation.StratifiedKFold(targetdata, trn_params['n_folds'])\n",
    "    CVO = list(CVO)\n",
    "    \n",
    "    # from each class an integer -> target max sparse\n",
    "    targetdata_sparse = np_utils.to_categorical(targetdata)\n",
    "    \n",
    "    for ifold in range(trn_params['n_folds']):\n",
    "        train_id, test_id = CVO[ifold]\n",
    "\n",
    "        # normalize data based in train set\n",
    "        if trn_params['norm'] == 'mapstd':\n",
    "            scaler = preprocessing.StandardScaler().fit(inputdata[train_id,:])\n",
    "        elif trn_params['norm'] == 'mapstd_rob':\n",
    "            scaler = preprocessing.RobustScaler().fit(inputdata[train_id,:])\n",
    "        elif trn_params['norm'] == 'mapminmax':\n",
    "            scaler = preprocessing.MinMaxScaler().fit(inputdata[train_id,:])\n",
    "        \n",
    "        if trn_params['norm'] != \"none\":\n",
    "            norm_inputdata = scaler.transform(inputdata)\n",
    "        else:\n",
    "            norm_inputdata = inputdata\n",
    "         \n",
    "        \n",
    "        classifiers[ifold] = {}\n",
    "        trn_desc[ifold] = {}\n",
    "        pcds[ifold] = {}\n",
    "        \n",
    "        \n",
    "    return [pcds,classifiers,trn_desc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCD Cooperative Extractor\n",
      "trn_params:  {'nesterov': True, 'learning_rate': 0.01, 'n_inits': 2, 'batch_size': 3, 'n_epochs': 1000, 'train_verbose': False, 'learning_decay': 0.0001, 'momentum': 0.9, 'n_folds': 2, 'norm': 'mapstd', 'n_pcds': 3}\n"
     ]
    }
   ],
   "source": [
    "# Extract PCD Cooperative\n",
    "trn_params = {}\n",
    "trn_params['n_folds'] = 2\n",
    "trn_params['n_inits'] = 2\n",
    "trn_params['n_pcds'] = 3\n",
    "trn_params['norm'] = 'mapstd'\n",
    "trn_params['learning_rate'] = 0.01\n",
    "trn_params['learning_decay'] = 1e-4\n",
    "trn_params['momentum'] = 0.9\n",
    "trn_params['nesterov'] = True\n",
    "trn_params['train_verbose'] = False\n",
    "trn_params['n_epochs'] = 1000\n",
    "trn_params['batch_size'] = 3\n",
    "\n",
    "\n",
    "[pcds,classifiers,trn_desc] = pcdc_extractor(data,trgt, trn_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
